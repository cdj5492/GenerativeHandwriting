{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XML files: 100%|██████████| 1000/1000 [00:07<00:00, 135.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "        [-0.0137,  0.3699,  0.0000,  1.0000],\n",
      "        [-0.0822,  0.4384,  0.0000,  1.0000],\n",
      "        ...,\n",
      "        [ 0.1233,  0.2329,  0.0000,  1.0000],\n",
      "        [-0.0685,  0.0274,  0.0000,  1.0000],\n",
      "        [ 0.0411,  0.0548,  0.0000,  1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "from util.createUnlabeledTrainingData import generate_training_data\n",
    "import glob\n",
    "\n",
    "strokes_dir = \"../lineStrokes-all/lineStrokes\"\n",
    "file_list = glob.glob(strokes_dir + \"/**/*.xml\", recursive=True)\n",
    "file_list = file_list[:1000] # limit to 1000 files for testing\n",
    "training_data = generate_training_data(file_list)\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "\n",
    "class UnlabelledLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=128, num_layers=2, num_mixtures=2):\n",
    "        super(UnlabelledLSTM, self).__init__()\n",
    "        self.num_mixtures = num_mixtures\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_mixtures*5)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(out.size(0), out.size(1), self.num_mixtures, 5)\n",
    "\n",
    "        pi = out[..., 0]\n",
    "        mu = out[..., 1:3]\n",
    "        sigma = out[..., 3:5]\n",
    "        pi = F.softmax(pi, dim=-1)\n",
    "        sigma = torch.exp(sigma)\n",
    "        return pi, mu, sigma, hidden\n",
    "\n",
    "        # x, hidden = self.lstm(x)\n",
    "        # x = self.fc(x)\n",
    "        # # first two outputs are between -1 and 1, so apply tanh\n",
    "        # x[:, :2, :] = torch.tanh(x[:, :2, :])\n",
    "        # # apply softmax to last two outputs\n",
    "        # x[:, -2:, :] = torch.softmax(x[:, -2:, :], dim=-1)\n",
    "        # return x, hidden\n",
    "\n",
    "def mdn_loss(pi, mu, sigma, target):\n",
    "    # target: (batch, seq_len, 2)\n",
    "    # Compute the probability for each mixture component\n",
    "    # First, expand target to (batch, seq_len, num_mixtures, 2)\n",
    "    target = target.unsqueeze(2).expand_as(mu)\n",
    "    # Compute the Gaussian probability density for each dimension independently\n",
    "    norm = 1.0 / (sqrt(2 * torch.pi) * sigma)\n",
    "    exp_term = torch.exp(-0.5 * ((target - mu) / sigma)**2)\n",
    "    prob = norm * exp_term  # (batch, seq_len, num_mixtures, 2)\n",
    "    # Assuming independence between dx and dy, multiply probabilities for both dimensions\n",
    "    prob = prob[..., 0] * prob[..., 1]  # (batch, seq_len, num_mixtures)\n",
    "    # Weight by mixing coefficients and sum over mixtures\n",
    "    prob = pi * prob\n",
    "    prob = torch.sum(prob, dim=-1)  # (batch, seq_len)\n",
    "    # Negative log likelihood\n",
    "    nll = -torch.log(prob + 1e-8)  # avoid log(0)\n",
    "    return torch.mean(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def sample_from_mdn(pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Given mixture parameters from one time step:\n",
    "      - pi: shape (num_mixtures,)\n",
    "      - mu: shape (num_mixtures, 2) for dx and dy\n",
    "      - sigma: shape (num_mixtures, 2) for dx and dy\n",
    "    Sample a mixture component according to pi and then draw a sample from its Gaussian.\n",
    "    Returns: (dx, dy) as a 2-element tensor.\n",
    "    \"\"\"\n",
    "    # Sample one mixture index according to the probabilities in pi.\n",
    "    # Make sure pi is a proper probability vector (it should be after softmax).\n",
    "    mixture_idx = torch.multinomial(pi, num_samples=1).item()\n",
    "    # Sample from the corresponding Gaussian for dx and dy.\n",
    "    # Here we assume dx and dy are independent.\n",
    "    sample = mu[mixture_idx] + sigma[mixture_idx] * torch.randn(2)\n",
    "    return sample  # tensor of shape (2,)\n",
    "\n",
    "def generate_sequence(model_d, length=100):\n",
    "    \"\"\"\n",
    "    Uses the MDN model to generate a new sequence of stroke commands.\n",
    "    The model outputs MDN parameters (pi, mu, sigma) for dx and dy.\n",
    "    For simplicity, we assume the pen is always \"down\" (pen state = [0, 1]).\n",
    "    \n",
    "    Returns a tensor of shape (length, 4), where each row is [dx, dy, 0, 1].\n",
    "    \"\"\"\n",
    "    # copy model and put it on the cpu\n",
    "    model = UnlabelledLSTM()\n",
    "    model.load_state_dict(model_d.state_dict())\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    # Start with an initial input vector.\n",
    "    # You might choose a different starting point depending on your training.\n",
    "    input_seq = torch.zeros(1, 1, 4)  # (batch=1, seq_len=1, feature_dim=4)\n",
    "    hidden = None  # carry over the hidden state for generation\n",
    "    output_seq = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            # Get the MDN outputs; note that our model returns (pi, mu, sigma, hidden)\n",
    "            pi, mu, sigma, hidden = model(input_seq, hidden)\n",
    "            pi = pi\n",
    "            mu = mu\n",
    "            sigma = sigma\n",
    "\n",
    "            # For our 1-step input, pick the outputs for the current (and only) time step.\n",
    "            # Each of these has shape: \n",
    "            #   pi: (1, 1, num_mixtures)\n",
    "            #   mu: (1, 1, num_mixtures, 2)\n",
    "            #   sigma: (1, 1, num_mixtures, 2)\n",
    "            pi_t = pi[0, -1]       # shape: (num_mixtures,)\n",
    "            mu_t = mu[0, -1]       # shape: (num_mixtures, 2)\n",
    "            sigma_t = sigma[0, -1] # shape: (num_mixtures, 2)\n",
    "            \n",
    "            # Sample a dx, dy from the MDN distribution.\n",
    "            sample = sample_from_mdn(pi_t, mu_t, sigma_t)  # (2,)\n",
    "            dx, dy = sample[0].item(), sample[1].item()\n",
    "            \n",
    "            # Determine pen state.\n",
    "            # In the original code you compared two outputs to decide pen up/down.\n",
    "            # Here, since our model only predicts dx and dy, we'll assume the pen is down.\n",
    "            # (If you later extend your model to predict pen state, update this accordingly.)\n",
    "            pen_state = [0, 1]  # [pen up, pen down]\n",
    "            \n",
    "            # Form the next output vector.\n",
    "            next_output = torch.tensor([dx, dy, pen_state[0], pen_state[1]], dtype=torch.float32)\n",
    "            output_seq.append(next_output.unsqueeze(0))  # shape (1, 4)\n",
    "            \n",
    "            # Feed this as the next input. It must have shape (1, 1, 4).\n",
    "            input_seq = next_output.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "    # Stack the outputs into a sequence: shape (length, 4)\n",
    "    return torch.cat(output_seq, dim=0)\n",
    "\n",
    "def convert_to_global_coords(seq, starting_point=(0.0, 0.0)):\n",
    "    \"\"\"\n",
    "    Converts a sequence of relative stroke commands (dx, dy, _, _)\n",
    "    to a sequence of global coordinates (x, y).\n",
    "    Assumes the starting point is (0, 0).\n",
    "    \"\"\"\n",
    "    global_seq = []\n",
    "    prev_x, prev_y = starting_point\n",
    "    # Loop through each step; seq is expected to be a tensor of shape (length, 4)\n",
    "    for step in seq:\n",
    "        # Here we ignore the pen state values (last two entries).\n",
    "        dx, dy = step[0].item(), step[1].item()\n",
    "        x = prev_x + dx\n",
    "        y = prev_y + dy\n",
    "        global_seq.append((x, y))\n",
    "        prev_x, prev_y = x, y\n",
    "    return global_seq\n",
    "\n",
    "# takes a given model and length and generates a sequence and shows it all in one go\n",
    "def plot_model(model, length=100, device='cpu'):\n",
    "    generated_seq = generate_sequence(model, length=length)\n",
    "    global_seq = convert_to_global_coords(generated_seq)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([x for x, _ in global_seq], [y for _, y in global_seq])\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "Epoch 1/50, Loss: -1.5733699967283756\n",
      "Epoch 2/50, Loss: -2.104852893397212\n",
      "Epoch 3/50, Loss: -2.1682400143146516\n",
      "Epoch 4/50, Loss: -2.2386750120222567\n",
      "Epoch 5/50, Loss: -2.271404272735119\n",
      "Epoch 6/50, Loss: -2.3538530164957048\n",
      "Epoch 7/50, Loss: -2.3409349619150164\n",
      "Epoch 8/50, Loss: -2.3867375425696373\n",
      "Epoch 9/50, Loss: -2.4780032250881194\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for UnlabelledLSTM:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([25, 128]) from checkpoint, the shape in current model is torch.Size([10, 128]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([25]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     39\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_filepath)\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(training_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 99\u001b[0m, in \u001b[0;36mplot_model\u001b[0;34m(model, length, device)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_model\u001b[39m(model, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 99\u001b[0m     generated_seq \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     global_seq \u001b[38;5;241m=\u001b[39m convert_to_global_coords(generated_seq)\n\u001b[1;32m    101\u001b[0m     fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mgenerate_sequence\u001b[0;34m(model_d, length)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# copy model and put it on the cpu\u001b[39;00m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m UnlabelledLSTM()\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UnlabelledLSTM:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([25, 128]) from checkpoint, the shape in current model is torch.Size([10, 128]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([25]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "# Training!\n",
    "use_modelfile = False\n",
    "model_filepath = \"../models/model_unlabeled2.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device:\", device)\n",
    "\n",
    "if use_modelfile:\n",
    "    model = UnlabelledLSTM(num_mixtures=5).to(device)\n",
    "    model.load_state_dict(torch.load(model_filepath, weights_only=True))\n",
    "else:\n",
    "    model = UnlabelledLSTM(num_mixtures=5).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    for seq in training_data:\n",
    "        # skip too short sequences\n",
    "        if seq.size(0) < 2:\n",
    "            continue\n",
    "\n",
    "        input_seq = seq[:-1].unsqueeze(0).to(device)\n",
    "        target_seq = seq[1:, :2].unsqueeze(0).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pi, mu, sigma, _ = model(input_seq)\n",
    "        loss = mdn_loss(pi, mu, sigma, target_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), model_filepath)\n",
    "        plot_model(model)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[108], line 99\u001b[0m, in \u001b[0;36mplot_model\u001b[0;34m(model, length, device)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_model\u001b[39m(model, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 99\u001b[0m     generated_seq \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     global_seq \u001b[38;5;241m=\u001b[39m convert_to_global_coords(generated_seq)\n\u001b[1;32m    101\u001b[0m     fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n",
      "Cell \u001b[0;32mIn[108], line 60\u001b[0m, in \u001b[0;36mgenerate_sequence\u001b[0;34m(model_d, length)\u001b[0m\n\u001b[1;32m     57\u001b[0m sigma_t \u001b[38;5;241m=\u001b[39m sigma[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# shape: (num_mixtures, 2)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Sample a dx, dy from the MDN distribution.\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43msample_from_mdn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_t\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (2,)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m dx, dy \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(), sample[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Determine pen state.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# In the original code you compared two outputs to decide pen up/down.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Here, since our model only predicts dx and dy, we'll assume the pen is down.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# (If you later extend your model to predict pen state, update this accordingly.)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[108], line 17\u001b[0m, in \u001b[0;36msample_from_mdn\u001b[0;34m(pi, mu, sigma)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mGiven mixture parameters from one time step:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m  - pi: shape (num_mixtures,)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mReturns: (dx, dy) as a 2-element tensor.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Sample one mixture index according to the probabilities in pi.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Make sure pi is a proper probability vector (it should be after softmax).\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m mixture_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Sample from the corresponding Gaussian for dx and dy.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Here we assume dx and dy are independent.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m sample \u001b[38;5;241m=\u001b[39m mu[mixture_idx] \u001b[38;5;241m+\u001b[39m sigma[mixture_idx] \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "plot_model(model.to('cpu'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
